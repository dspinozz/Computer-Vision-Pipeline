# Computer Vision Pipeline

[![Python](https://img.shields.io/badge/python-3.11+-blue)]()
[![License](https://img.shields.io/badge/license-MIT-green)]()

A production-grade vision processing pipeline that extracts embeddings, generates captions, detects objects, and creates pixel masks for large image collections. Supports semantic search via Qdrant and structured queries via PostgreSQL.

## ğŸ¯ Features

- **Semantic Search**: Find images by natural language queries
- **Object Detection**: Identify and localize objects with bounding boxes
- **Mask Generation**: Pixel-precise segmentation for interactive regions
- **Metadata Extraction**: Rich captions, tags, and scene descriptions
- **Production Infrastructure**: FastAPI, Qdrant, PostgreSQL, Docker

## ğŸ“Š Pipeline Results

Processed **36,497 images** from Places365 dataset:

| Component | Model | Output |
|-----------|-------|--------|
| Embeddings | SigLIP So400m (1152-dim) | 36,497 vectors |
| Captions | Qwen2.5-VL-3B-Instruct | Avg 199 chars/image |
| Tags | SigLIP + Taxonomy (155 tags) | 10 ranked tags/image |
| Object Detection | GroundingDINO | 77,344 objects |
| Masks | SAM2 | 77,334 pixel masks |

## ğŸ“ Data Source

This pipeline was developed and tested using the **Places365 validation set**:

- **Dataset**: [Places365-Standard](http://places2.csail.mit.edu/download.html)
- **Split**: val_256 (validation images, 256x256)
- **Size**: 36,500 images across 365 scene categories
- **License**: Creative Commons Attribution (CC BY)

To recreate:

```bash
# Download Places365 validation set
python scripts/download_places365.py --output-dir data/places365

# Or manually download from:
# http://data.csail.mit.edu/places/places365/val_256.tar
```

The pipeline works with any image collection - just point it to your image directory.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Input Images                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Lane A: Metadata                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  SigLIP  â”‚  â”‚ Qwen2.5-VL   â”‚  â”‚   SigLIP     â”‚               â”‚
â”‚  â”‚Embedding â”‚  â”‚  Captioning  â”‚  â”‚ Tag Scoring  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚       â”‚               â”‚                 â”‚                        â”‚
â”‚       â–¼               â–¼                 â–¼                        â”‚
â”‚   1152-dim        Caption +         10 ranked                   â”‚
â”‚    vector        short + nouns        tags                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Lane B: Detection                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ GroundingDINOâ”‚         â”‚     SAM2     â”‚                      â”‚
â”‚  â”‚  Detection   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    Masks     â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚         â”‚                        â”‚                               â”‚
â”‚         â–¼                        â–¼                               â”‚
â”‚   Bounding boxes          Pixel masks +                         â”‚
â”‚   + labels                 centroids                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Storage Layer                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Qdrant  â”‚    â”‚ Postgres â”‚    â”‚  JSON Files  â”‚               â”‚
â”‚  â”‚ Vectors  â”‚    â”‚ Metadata â”‚    â”‚   (backup)   â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      FastAPI                                     â”‚
â”‚  GET /v1/images/search?q=...    Semantic search                 â”‚
â”‚  GET /v1/images/{id}            Image details                   â”‚
â”‚  GET /v1/stats                  Collection stats                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose
- NVIDIA GPU (for processing; serving is CPU-only)

### 1. Clone and Setup

```bash
git clone https://github.com/dspinozz/Computer-Vision-Pipeline.git
cd Computer-Vision-Pipeline

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Download Data

```bash
# Download Places365 validation set (~1GB)
python scripts/download_places365.py --output-dir data/places365
```

### 3. Start Infrastructure

```bash
docker compose up -d postgres qdrant
```

### 4. Process Images

```bash
# Run full pipeline (requires GPU)
python scripts/pipeline.py process --input-dir data/places365/val_256

# Import to vector database
python scripts/import_qdrant.py

# Migrate to PostgreSQL
python scripts/migrate_to_postgres.py
```

### 5. Start API

```bash
uvicorn api.main:app --host 0.0.0.0 --port 8000
```

### 6. Search

```bash
curl "http://localhost:8000/v1/images/search?q=mountain+landscape"
```

## ğŸ“ Project Structure

```
Computer-Vision-Pipeline/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py              # FastAPI application
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ schema.sql           # PostgreSQL schema
â”‚   â””â”€â”€ taxonomy.json        # Tag taxonomy (155 tags)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ pipeline.py          # Main processing pipeline
â”‚   â”œâ”€â”€ download_places365.py # Data download script
â”‚   â”œâ”€â”€ import_qdrant.py     # Vector import
â”‚   â””â”€â”€ migrate_to_postgres.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ demo.py              # Demo script
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile.api
â”œâ”€â”€ .env.example             # Environment template
â””â”€â”€ requirements.txt
```

## ğŸ”§ Models Used

| Model | Purpose | Size | Source |
|-------|---------|------|--------|
| SigLIP So400m | Embeddings + tags | 400M | [HuggingFace](https://huggingface.co/google/siglip-so400m-patch14-384) |
| Qwen2.5-VL-3B | Captioning | 3B | [HuggingFace](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) |
| GroundingDINO | Object detection | 172M | [GitHub](https://github.com/IDEA-Research/GroundingDINO) |
| SAM2 | Segmentation | 312M | [GitHub](https://github.com/facebookresearch/sam2) |

## ğŸ“Š Metadata Schema

Each image produces structured metadata:

```json
{
  "image_id": "uuid",
  "file_path": "data/places365/val_256/image.jpg",
  "file_hash": "sha256...",
  "width": 256,
  "height": 256,
  "caption": {
    "text": "Full VLM-generated description...",
    "short": "First 100 chars...",
    "extracted_nouns": ["mountain", "sky", "tree"]
  },
  "tags": [
    {"tag_id": "scene.mountain", "display": "Mountain", "confidence": 0.14}
  ],
  "objects": [
    {
      "label": "person",
      "confidence": 0.87,
      "box": {"x1": 0.1, "y1": 0.2, "x2": 0.3, "y2": 0.8},
      "mask_rle": "...",
      "mask_area": 1024,
      "centroid": [128.5, 192.3]
    }
  ]
}
```

## ğŸ¯ Use Cases

- **Semantic Image Search**: Find images by natural language description
- **Interactive Applications**: Clickable regions with pixel-precise masks
- **Content Moderation**: Detect and classify image content
- **Accessibility**: Generate alt-text and scene descriptions
- **Asset Libraries**: Search stock images by semantic similarity

## ğŸ“ˆ Performance

- **Processing speed**: ~2 images/sec (GPU)
- **Qdrant search**: <50ms for 36K vectors
- **API latency**: <100ms (cached embeddings)

## ğŸ”’ Security Note

Default passwords in `docker-compose.yml` are for **development only**. 
For production:
1. Copy `.env.example` to `.env`
2. Set secure passwords
3. Use `docker compose --env-file .env up`

## License

MIT
